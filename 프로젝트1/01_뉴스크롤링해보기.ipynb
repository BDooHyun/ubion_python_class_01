{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "11\n",
      "21\n",
      "31\n",
      "41\n",
      "51\n",
      "61\n",
      "71\n",
      "81\n",
      "91\n",
      "101\n",
      "111\n",
      "121\n",
      "131\n",
      "141\n",
      "151\n",
      "161\n",
      "171\n",
      "181\n",
      "191\n",
      "201\n",
      "211\n",
      "221\n",
      "231\n",
      "241\n",
      "251\n",
      "261\n",
      "271\n",
      "281\n",
      "291\n",
      "301\n",
      "311\n",
      "321\n",
      "331\n",
      "341\n",
      "351\n",
      "361\n",
      "371\n",
      "381\n",
      "391\n",
      "401\n",
      "411\n",
      "421\n",
      "431\n",
      "441\n",
      "451\n",
      "461\n",
      "471\n",
      "481\n",
      "491\n",
      "501\n",
      "511\n",
      "521\n",
      "531\n",
      "541\n",
      "551\n",
      "561\n",
      "571\n",
      "581\n",
      "591\n",
      "601\n",
      "611\n",
      "621\n",
      "631\n",
      "641\n",
      "651\n",
      "661\n",
      "671\n",
      "681\n",
      "691\n",
      "701\n",
      "711\n",
      "721\n",
      "731\n",
      "741\n",
      "751\n",
      "761\n",
      "771\n",
      "781\n",
      "791\n",
      "801\n",
      "811\n",
      "821\n",
      "831\n",
      "841\n",
      "851\n",
      "861\n",
      "871\n",
      "881\n",
      "891\n",
      "901\n",
      "911\n",
      "921\n",
      "931\n",
      "941\n",
      "951\n",
      "961\n",
      "971\n",
      "981\n",
      "991\n"
     ]
    }
   ],
   "source": [
    "# # -*- coding: utf-8 -*-\n",
    "# from bs4 import BeautifulSoup\n",
    "# from datetime import datetime\n",
    "# import requests\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# '''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "# < naver 뉴스 검색시 리스트 크롤링하는 프로그램 > _select사용\n",
    "# - 크롤링 해오는 것 : 링크,제목,신문사,날짜,내용요약본\n",
    "# - 날짜,내용요약본  -> 정제 작업 필요\n",
    "# - 리스트 -> 딕셔너리 -> df -> 엑셀로 저장 \n",
    "# '''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "\n",
    "# #각 크롤링 결과 저장하기 위한 리스트 선언 \n",
    "# title_text=[]\n",
    "# link_text=[]\n",
    "# source_text=[]\n",
    "# date_text=[]\n",
    "# contents_text=[]\n",
    "# result={}\n",
    "\n",
    "# #엑셀로 저장하기 위한 변수\n",
    "# RESULT_PATH ='C:/Users/duja1/OneDrive/문서/GitHub/ubion_python_class_01/프로젝트1/crawling_result'  #결과 저장할 경로\n",
    "# now = datetime.now() #파일이름 현 시간으로 저장하기\n",
    "\n",
    "# #날짜 정제화 함수\n",
    "# def date_cleansing(test):\n",
    "#     try:\n",
    "#         #지난 뉴스\n",
    "#         #머니투데이  10면1단  2018.11.05.  네이버뉴스   보내기  \n",
    "#         pattern = '\\d+.(\\d+).(\\d+).'  #정규표현식 \n",
    "    \n",
    "#         r = re.compile(pattern)\n",
    "#         match = r.search(test).group(0)  # 2018.11.05.\n",
    "#         date_text.append(match)\n",
    "        \n",
    "#     except AttributeError:\n",
    "#         #최근 뉴스\n",
    "#         #이데일리  1시간 전  네이버뉴스   보내기  \n",
    "#         pattern = '\\w* (\\d\\w*)'     #정규표현식 \n",
    "        \n",
    "#         r = re.compile(pattern)\n",
    "#         match = r.search(test).group(1)\n",
    "#         #print(match)\n",
    "#         date_text.append(match)\n",
    "\n",
    "\n",
    "# #내용 정제화 함수 \n",
    "# def contents_cleansing(contents):\n",
    "#     first_cleansing_contents = re.sub('<dl>.*?</a> </div> </dd> <dd>', '', \n",
    "#                                       str(contents)).strip()  #앞에 필요없는 부분 제거\n",
    "#     second_cleansing_contents = re.sub('<ul class=\"relation_lst\">.*?</dd>', '', \n",
    "#                                        first_cleansing_contents).strip()#뒤에 필요없는 부분 제거 (새끼 기사)\n",
    "#     third_cleansing_contents = re.sub('<.+?>', '', second_cleansing_contents).strip()\n",
    "#     contents_text.append(third_cleansing_contents)\n",
    "#     #print(contents_text)\n",
    "    \n",
    "\n",
    "# def crawler(maxpage,query,sort,s_date,e_date):\n",
    "\n",
    "#     s_from = s_date.replace(\".\",\"\")\n",
    "#     e_to = e_date.replace(\".\",\"\")\n",
    "#     page = 1  \n",
    "#     maxpage_t =(int(maxpage)-1)*10+1   # 11= 2페이지 21=3페이지 31=4페이지  ...81=9페이지 , 91=10페이지, 101=11페이지\n",
    "    \n",
    "#     while page <= maxpage_t:\n",
    "#         url = \"https://search.naver.com/search.naver?where=news&query=\" + query + \"&sort=\" + sort + \"&ds=\" + s_date + \"&de=\" + e_date + \"&nso=so%3Ar%2Cp%3Afrom\" + s_from + \"to\" + e_to + \"%2Ca%3A&start=\" + str(page) + \"&field=0\"  # 파라미터 추가\n",
    "        \n",
    "#         response = requests.get(url)\n",
    "#         html = response.text\n",
    "\n",
    "        \n",
    " \n",
    "#         #뷰티풀소프의 인자값 지정\n",
    "#         soup = BeautifulSoup(html, 'html.parser')\n",
    " \n",
    "#         #<a>태그에서 제목과 링크주소 추출\n",
    "#         atags = soup.select('.news_tit')\n",
    "#         for atag in atags:\n",
    "#             title_text.append(atag.text)     #제목\n",
    "#             link_text.append(atag['href'])   #링크주소\n",
    "            \n",
    "#         #신문사 추출\n",
    "#         source_lists = soup.select('.info_group > .press')\n",
    "#         for source_list in source_lists:\n",
    "#             source_text.append(source_list.text)    #신문사\n",
    "        \n",
    "#         #날짜 추출 \n",
    "#         date_lists = soup.select('.info_group > span.info')\n",
    "#         for date_list in date_lists:\n",
    "#             # 1면 3단 같은 위치 제거\n",
    "#             if date_list.text.find(\"면\") == -1:\n",
    "#                 date_text.append(date_list.text)\n",
    "        \n",
    "#         #본문요약본\n",
    "#         contents_lists = soup.select('.news_dsc')\n",
    "#         for contents_list in contents_lists:\n",
    "#             contents_cleansing(contents_list) #본문요약 정제화\n",
    "        \n",
    "\n",
    "#         #모든 리스트 딕셔너리형태로 저장\n",
    "#         result= {\"date\" : date_text , \"title\":title_text ,  \"source\" : source_text ,\"contents\": contents_text ,\"link\":link_text }  \n",
    "#         print(page)\n",
    "        \n",
    "#         df = pd.DataFrame(result)  #df로 변환\n",
    "#         page += 10\n",
    "    \n",
    "    \n",
    "#     # 새로 만들 파일이름 지정\n",
    "#     outputFileName = '%s-%s-%s  %s시 %s분 %s초 merging.xlsx' % (now.year, now.month, now.day, now.hour, now.minute, now.second)\n",
    "#     df.to_excel(RESULT_PATH+outputFileName,sheet_name='sheet1')\n",
    "    \n",
    "    \n",
    "\n",
    "# def main():\n",
    "#     info_main = input(\"=\"*50+\"\\n\"+\"입력 형식에 맞게 입력해주세요.\"+\"\\n\"+\" 시작하시려면 Enter를 눌러주세요.\"+\"\\n\"+\"=\"*50)\n",
    "    \n",
    "#     maxpage = input(\"최대 크롤링할 페이지 수 입력하시오: \")  \n",
    "#     query = input(\"검색어 입력: \")  \n",
    "#     sort = input(\"뉴스 검색 방식 입력(관련도순=0  최신순=1  오래된순=2): \")    #관련도순=0  최신순=1  오래된순=2\n",
    "#     s_date = input(\"시작날짜 입력(2019.01.04):\")  #2019.01.04\n",
    "#     e_date = input(\"끝날짜 입력(2019.01.05):\")   #2019.01.05\n",
    "    \n",
    "#     crawler(maxpage,query,sort,s_date,e_date) \n",
    "    \n",
    "# main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "# import openpyxl\n",
    "# from openpyxl import Workbook\n",
    "# import pandas as pd\n",
    "# from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "\n",
    "# wb = Workbook()\n",
    "# sheet = wb.active\n",
    "# sheet.append(['발행일','언론사','기사제목','URL','내용'])\n",
    "\n",
    "# p = {'중앙일보','동아일보','한겨레','오마이뉴스언론사 선정','조선일보','경향일보','MBC','KBS','SBS'}\n",
    "# headers = { \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/98.0.4758.102\" }\n",
    "\n",
    "# #1, 11, 21....91\n",
    "# for page in range(1,70,10):\n",
    "\n",
    "#     #Get 요청, naver 서버에 대화 시도\n",
    "#     response = requests.get(f'https://search.naver.com/search.naver?where=news&sm=tab_pge&query=%EC%9E%A5%EC%95%A0%EC%9D%B8%20%EC%9D%B4%EB%8F%99%EA%B6%8C%20%EC%8B%9C%EC%9C%84&sort=0&photo=0&field=0&pd=3&ds=2022.01.01&de=2022.01.31&cluster_rank=51&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:from20220101to20220131,a:all&start={page}')\n",
    "   \n",
    "\n",
    "#     #네이버에서 html 제공, text 메소드로 태그 내 텍스트만 추출\n",
    "#     html = response.text\n",
    "\n",
    "#     #html 번역선생님으로 수프 만듦\n",
    "#     soup = BeautifulSoup(html, 'html.parser')\n",
    "#     news = soup.select('div.news_wrap.api_ani_send')\n",
    "#     for n in news:\n",
    "#         title = n.select_one('a.news_tit').text\n",
    "#         press = n.select_one('a.info.press').text\n",
    "#         #date = n.select_one('span.info').text\n",
    "#         if press in p:\n",
    "#             try:\n",
    "#                 url = n.select_one('div.info_group > a:nth-of-type(2)')['href']\n",
    "#             except: \n",
    "#                 continue\n",
    "#             article = requests.get(url,headers = headers)\n",
    "#             article_html = BeautifulSoup(article.text,\"html.parser\")\n",
    "            \n",
    "#             try:\n",
    "#                 date = article_html.select_one('div.sponsor > span.t11').text  \n",
    "#             except:\n",
    "#                 continue\n",
    "#             try:\n",
    "#                 content = article_html.select_one('div.content > div#articleBody > div#articleBodyContents').text\n",
    "#             except:\n",
    "#                 continue\n",
    "#             sheet.append([date, press, title,url, content])\n",
    "#             #sheet.append([date, press, title,url])\n",
    "#         else:\n",
    "#             continue\n",
    "\n",
    "\n",
    "\n",
    "# #간격조절\n",
    "# sheet.column_dimensions['A'].width = 10\n",
    "# sheet.column_dimensions['B'].width = 10\n",
    "# sheet.column_dimensions['C'].width = 30\n",
    "# sheet.column_dimensions['D'].width = 40\n",
    "# sheet.column_dimensions['E'].width = 50\n",
    "# wb.save(filename='1월.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "\n",
    "# # 크롤링할 페이지 URL\n",
    "# url = 'https://news.naver.com/main/main.nhn?mode=LSD&mid=shm&sid1=101'\n",
    "\n",
    "# # 추출할 날짜 설정\n",
    "# target_date = '2022-01-01'\n",
    "\n",
    "# # HTTP GET 요청\n",
    "# response = requests.get(url)\n",
    "\n",
    "# # 응답 데이터 파싱\n",
    "# soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# # 데이터 저장을 위한 리스트\n",
    "# data = []\n",
    "\n",
    "# # 뉴스 기사 제목과 날짜 추출\n",
    "# titles = soup.select('.cluster_text > a')\n",
    "# dates = soup.select('.cluster_text > .t11')\n",
    "\n",
    "# # 기사 제목 추출\n",
    "# for title, date in zip(titles, dates):\n",
    "#     news_date = date.text.strip()\n",
    "#     if news_date == target_date:\n",
    "#         data.append({\n",
    "#             '날짜': news_date,\n",
    "#             '제목': title.text\n",
    "#         })\n",
    "\n",
    "# # 데이터프레임 생성\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # CSV 파일로 저장\n",
    "# df.to_csv('출력파일명.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# 크롤링할 페이지 URL\n",
    "url = 'https://news.daum.net/economic#1'\n",
    "\n",
    "# 크롤링할 기간 설정\n",
    "start_date = '2023-01-01'\n",
    "end_date = '2023-01-31'\n",
    "\n",
    "# HTTP GET 요청\n",
    "response = requests.get(url)\n",
    "\n",
    "# 응답 데이터 파싱\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# 데이터 저장을 위한 리스트\n",
    "data = []\n",
    "\n",
    "# 뉴스 기사 제목과 날짜 추출\n",
    "articles = soup.select('.list_news2 > li')\n",
    "\n",
    "# 기사 제목과 날짜 추출\n",
    "for article in articles:\n",
    "    title = article.select_one('.tit_thumb > a')\n",
    "    date = article.select_one('.info_news > span:nth-child(2)')\n",
    "\n",
    "    print(\"title\",title)\n",
    "    print(\"Date\",date)\n",
    "    \n",
    "    if title and date:\n",
    "        news_title = title.get_text(strip=True)\n",
    "        news_date = date.get_text(strip=True)\n",
    "        \n",
    "\n",
    "        data.append({\n",
    "                '제목': news_title,\n",
    "                '날짜': news_date\n",
    "            })\n",
    "        # if start_date <= news_date <= end_date:\n",
    "        #     data.append({\n",
    "        #         '제목': news_title,\n",
    "        #         '날짜': news_date\n",
    "        #     })\n",
    "\n",
    "print(data)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# CSV 파일로 저장\n",
    "df.to_csv('다음경제뉴스.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# 크롤링할 페이지 URL\n",
    "url = 'https://www.mk.co.kr/news/economy/'\n",
    "\n",
    "# 크롤링할 기간 설정\n",
    "start_date = '2022-01-01'\n",
    "end_date = '2023-05-31'\n",
    "\n",
    "# HTTP GET 요청\n",
    "response = requests.get(url)\n",
    "\n",
    "# 응답 데이터 파싱\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# 데이터 저장을 위한 리스트\n",
    "data = []\n",
    "\n",
    "# 뉴스 기사 제목과 날짜 추출\n",
    "articles = soup.select('.list_news > li')\n",
    "\n",
    "# 기사 제목과 날짜 추출\n",
    "for article in articles:\n",
    "    title = article.select_one('.tit_art')\n",
    "    date = article.select_one('.sub_time')\n",
    "\n",
    "    if title and date:\n",
    "        news_title = title.text.strip()\n",
    "        news_date = date.text.strip()\n",
    "\n",
    "        if start_date <= news_date <= end_date:\n",
    "            data.append({\n",
    "                '제목': news_title,\n",
    "                '날짜': news_date\n",
    "            })\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# CSV 파일로 저장\n",
    "df.to_csv('한국경제뉴스.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# 크롤링할 페이지 URL\n",
    "url = 'https://www.mk.co.kr/news/economy/'\n",
    "\n",
    "# 크롤링할 기간 설정\n",
    "start_date = '2023-01-01'\n",
    "end_date = '2023-01-31'\n",
    "\n",
    "# HTTP GET 요청\n",
    "response = requests.get(url)\n",
    "\n",
    "# 응답 데이터 파싱\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# 데이터 저장을 위한 리스트\n",
    "data = []\n",
    "\n",
    "# 뉴스 기사 추출\n",
    "articles = soup.select('.list_news > li')\n",
    "\n",
    "# 기사 제목과 내용 추출\n",
    "for article in articles:\n",
    "    title = article.select_one('.tit_art')\n",
    "    date = article.select_one('.sub_time')\n",
    "    content_url = article.select_one('.tit_art > a')['href']\n",
    "\n",
    "    if title and date:\n",
    "        news_title = title.text.strip()\n",
    "        news_date = date.text.strip()\n",
    "\n",
    "        if start_date <= news_date <= end_date:\n",
    "            # 내용 크롤링\n",
    "            content_response = requests.get(content_url)\n",
    "            content_soup = BeautifulSoup(content_response.text, 'html.parser')\n",
    "            news_content = content_soup.select_one('.art_txt').text.strip()\n",
    "\n",
    "            data.append({\n",
    "                '제목': news_title,\n",
    "                '날짜': news_date,\n",
    "                '내용': news_content\n",
    "            })\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# CSV 파일로 저장\n",
    "df.to_csv('한국경제뉴스.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
